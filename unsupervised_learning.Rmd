---
title: "Homework 10"
author: "Robbie Walsh"
date: "10/24/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The purpose of this analysis is to analyze a dataset on transactions for a UK-based online retail store.  We will perform clustering as an unsupervised learning problem to attempt to meaningfully segment the customers by the recency and duration of their interaction with the company, the frequency of their purchases/orders, and the amount they spent (amounts are in Sterling pounds).  We will use both k-means and hierarchical clustering as well as PCA to attempt to create segments.  The results are that outliers and extreme observations are easily identified using the unsupervised learning algorithms we explore.  There are several customers who spend large amounts, large amounts per purchase, and several other dimensions that cause these types of customers to be assigned their own cluster while the less extreme observations are predominantly assigned to one large cluster.

# Data

This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.  Some data cleaning is conducted to remove "unspecified" countries, remove returns and cancellations, remove duplicate IDs, and aggregate the data to get information by each customer instead of by each transaction.

```{r,include=F}
library(tidyverse)
library(readxl)                                      # you may need to install the 'readxl' package first
eretail = read_excel("C:/Users/robbi/OneDrive/Documents/data_min/Online Retail.xlsx")
dim(eretail)
names(eretail)

eretail = eretail[eretail$Country != "Unspecified",] # remove 'unspecified' country
eretail = eretail[eretail$Quantity > 0,]             # remove returns/cancellations

IDtab = table(eretail$Country, eretail$CustomerID)   # crosstab country by customer ID
IDtab = apply(IDtab >0, 2, sum)                      # is any customer ID duplicated across countries?
duplicateIDs = names(IDtab[IDtab > 1])               # duplicate IDs to clean up
eretail = eretail[!is.element(eretail$CustomerID, duplicateIDs),]
rm(IDtab)

eretail$InvoiceMth = substr(eretail$InvoiceDate, 1, 7)         # extract month of invoice
eretail = eretail[as.vector(eretail$InvoiceMth) != "2011-12",] # remove December 2011 as it only covers first week

eretail$Amount = eretail$Quantity * eretail$UnitPrice           # compute amount per invoice item

eaggr = aggregate(Amount~Country+CustomerID, data=eretail, sum) # compute aggregate amount spent per customer
row.names(eaggr) = eaggr$CustomerID
eaggr = eaggr[,-2]
eaggr = cbind(eaggr, aggregate(InvoiceMth~CustomerID, data=eretail, min)[,-1]) # 1st month of customer interaction
names(eaggr)[3] = "FirstMth"
eaggr = cbind(eaggr, aggregate(InvoiceMth~CustomerID, data=eretail, max)[,-1]) # last month of cust. interaction
names(eaggr)[4] = "LastMth"

# relabel months and compute duration of customer interaction
eaggr$FirstMth = as.factor(eaggr$FirstMth)
eaggr$LastMth = as.factor(eaggr$LastMth)
levels(eaggr$FirstMth) = 1:12
levels(eaggr$LastMth) = 1:12
eaggr$Months = as.numeric(eaggr$LastMth) - as.numeric(eaggr$FirstMth) + 1

eaggr = cbind(eaggr, apply( table(eretail$CustomerID, eretail$InvoiceMth) , 1, sum ) )
names(eaggr)[6] = "Purchases"

# Some useful statistics (which you may or may not decide to use)
eaggr$Amount.per.Purchase = eaggr$Amount / eaggr$Purchases
eaggr$Purchases.per.Month = eaggr$Purchases / eaggr$Months
eaggr$Amount.per.Month = eaggr$Amount / eaggr$Months

eaggr[1:30,]
row.names(eaggr) <- seq.int(nrow(eaggr))
```

# Analysis

## K Nearest Neighbors

First we will use K nearest neighbors to attempt the segment the customers.  K nearest neighbors creates k centroids in the feature space and assigns a point to a cluster based on which centroid it is closest to.  Close is defined using Euclidian distance.

```{r}
set.seed(5)
km.out <- kmeans(eaggr %>% select(-Country),centers = 5,nstart = 20)
kdata <- eaggr %>% mutate(cluster = as.factor(km.out$cluster))
ggplot(kdata,aes(x=Amount,y=Purchases,color=cluster)) + geom_point() + labs(title = "Figure 1: Cluster Vizualization, Color for each Cluster",subtitle = "K=5")
km.out <- kmeans(eaggr %>% select(-Country),centers = 3,nstart = 20)
kdata <- eaggr %>% mutate(cluster = as.factor(km.out$cluster))
ggplot(kdata,aes(x=Amount,y=Purchases,color=cluster)) + geom_point() + labs(title = "Figure 2: Cluster Vizualization, color for each Cluster",subtitle = "K=3")
km.out <- kmeans(eaggr %>% select(-Country),centers = 8,nstart = 20)
kdata <- eaggr %>% mutate(cluster = as.factor(km.out$cluster))
ggplot(kdata,aes(x=Amount,y=Purchases,color=cluster)) + geom_point() + labs(title = "Figure 3: Cluster Vizualization, Color for each Cluster",subtitle = "K=8")
```
Looking at several choices of k, and plotting Purchases vs Amount, we see that Amount is the key clustering variable.  Across several choices of k, we see that the clusters are created almost exclusively by the total Amount that the customer spent, with the size of the clusters varying based on what value of k is chosen.  This is likely due to the fact that we have not performed any standardization of the variables, and since Amount is on a larger scale than all the other variables, the Euclidean distance between points is dominated by the Amount variable.  While this makes the analysis simple, segmentation by the total amount a customer spends is an intuitive and powerful segmentation.  If there are a few customers who tend to spend a lot, then special attention should be paid to them to retain them and keep them spending at your store since they drive an outsize amount of your profits.  Less attention needs to be paid to customers who spend less at your store and there is less consequence if they spend less or switch to a different store.  We will focus on analysis and segmentation beyond the amount variable in the next two algorithms.


## Hierarchical Clustering

The next algorithm we will use is hierarchical clustering.  Hierarchical clustering groups observations based on how similar/dissimilar they are and creates a dendogram to visualize the hierarchy of these relationships.  This allows the analyst to choose the number of clusters they want to ultimately create after the algorithm has been run. 

In this analysis we will run complete linkage, which gives us maximal intercluster dissimilarity. The algorithm will compute all the pairwise dissimilarities between the observations in the first cluster and the observations in another cluster, and record the largest of these dissimilarities.  While this choice was somewhat arbitrary, it tended to result in more splits.

Since we saw in the kNN section that the amount variable can be overly important, we will perform scaling of the data before passing it into the hierarchical clustering algorithm to generate different results.  We will also remove the variables that are non-numeric (Country, and the first and last month variable since they are coded as factors).  An initial run of the algorithm indicates that the first row of the data is very unique and is likely causing analysis to be skewed by how much of an outlier it is.  A close analysis of the data indicates that there was one purchase made, but it was for a very large amount, putting this observation in the unique position of having very high "amount per" numbers and a high amount, with a low amount of purchases.  While this is noteworthy, we will remove this observation to generate more insight from this algorithm.  

```{r}
set.seed(5)
hclust_in <- dist(scale(eaggr %>% select(-Country,-FirstMth,-LastMth) %>% filter(Amount.per.Purchase < 70000)))
hc.complete <- hclust(hclust_in,method="complete")
plot(hc.complete,main="Figure 4: Complete Linkage Dendogram", xlab="", sub="",  cex=.9)

hclust <- eaggr %>% filter(Amount.per.Purchase<70000) %>% mutate(cluster = as.factor(cutree(hc.complete,5)))
ggplot(hclust,aes(x=Amount,y=Purchases,color=cluster)) + geom_point() + labs(title = "Figure 5: HClust Results, Color for each Cluster",subtitle = "Amount/Purchase<70000 excluded")
ggplot(hclust %>% filter(Amount.per.Purchase<10000),aes(x=Amount.per.Purchase,y=Amount.per.Month,color=cluster)) + geom_point() + labs(title = "Figure 6: HClust Results, Color for each Cluster",subtitle = "Amount/Purchase<10000 excluded")
```
While figure 4 is difficult to read (no good way to make a graph with more than 4000 points legible) we can see that choosing a low (less than 20) number of clusters will result in many observations being in the same cluster.  This makes sense as many customers are relatively homogeneous and clustering analysis will pick up on outlier customers.  Cutting the tree at 5 (results similar for larger numbers of clusters and would take up too much space) and assigning each point to its respective cluster we can see which variables are important for this cluster.  While figure 5 shows that the raw numbers (scaled) of purchases and amount is important in determening the cluster, we see in figure 6 that the important variables are amount per month and amount per purchase.  Points in the extreme ends of figure 6 are in their own cluster (with an excluded point in its own cluster) indicating that these variables were particularly important in determening this cluster.  This segmentation would identify customers who spend a lot per purchase, and who spend a large amount per month.  This would allow the company to send targeted messages to each type of customer such as free shipping for the large amount per purchase.

## Principal Components Analysis

The final algorithm we will use is PCA, which creates linear combinations of the input variables that explain a good proportion of the variance in the data.  PCA creates principal components which are orthogonal to each other, which can be a useful property.

PCA works best when the input variables are scaled so we will perform similar data pre-processing as with hierarchical clustering, removing problematic variables.

```{r}
pca.out <- prcomp(eaggr %>% select(-Country,-FirstMth,-LastMth),scale. = T)
summary(pca.out)
pca.out
```
While PCA's output is more quantitative in nature than the previous two algorithms we looked at, we can still see view the factor loadings to see how each principal component would segment the data.  The first PC has the largest absolute values for amount per month, amount, and purchases, and they are all negative.  Customers who scored very negative on this component would be the biggest spenders and most likely to buy lots of items and spend a lot of money at the store.  The second PC has the largest absolute values for amount per purchase, and purchases, and they are in opposite directions, indicating that this will segment customers into groups that either spend a lot per purchase, or make a lot of purchases, effectively segmenting customers by whether they are occasional, big spenders, or frequent small spenders.  While some additional work would have to be done in order to operationalize PCA to create segments, it would be another useful way to identify potential segmentation schemes for customers.

# Conclusions

While it is difficult to assess the results of unsupervised learning because there are no target metrics or variables, the algorithms analyzed in this report appeared to be good at targeting outliers or particularly extreme observations in the dataset.  If the goal was to create several large segments of customers, then PCA likely would be the best algorithm to use in this case because it was the least prone to fixating on outliers and could divide the data along the direction of the most variance.

The first two algorithms appeared to focus on one or more variables when clustering the data, putting extreme observations into their own clusters and the many similar observations into a large cluster.  This indicates that for the online retail store's data, there are several types of customers worth paying attention to.  There are customers who spend large amounts, customers who spend a lot per purchase, customers who make lots of purchases, and several other types that were identified by one or more of the algorithms.  While the algorithms did not tend to find two or three types of customers, the algorithms were good at identifying extreme observations, which appear to be important in this type of data. 